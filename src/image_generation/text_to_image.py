import os
import json
import torch
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from PIL import Image
import numpy as np
from ..content_generation.llm_client import LLMClient

# å¯é€‰ä¾èµ–æ£€æŸ¥
try:
    from diffusers import (
        StableDiffusionPipeline, 
        StableDiffusionXLPipeline,
        DPMSolverMultistepScheduler,
        EulerAncestralDiscreteScheduler
    )
    from transformers import AutoTokenizer
    import accelerate
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False
    print("âš ï¸ å›¾åƒç”Ÿæˆä¾èµ–æœªå®‰è£…ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
    print("pip install -r requirements-image.txt")

class TextToImageGenerator:
    def __init__(self):
        self.llm_client = LLMClient()
        self.output_dir = "./outputs/images"
        os.makedirs(self.output_dir, exist_ok=True)
        self.diffusers_available = DIFFUSERS_AVAILABLE
        self.pipeline = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # é»˜è®¤æ¨¡å‹é…ç½®
        self.default_models = {
            "sd15": "runwayml/stable-diffusion-v1-5",
            "sdxl": "stabilityai/stable-diffusion-xl-base-1.0",
            "sd21": "stabilityai/stable-diffusion-2-1"
        }
        
        # é¢„å®šä¹‰é£æ ¼
        self.style_prompts = {
            "å†™å®": "photorealistic, highly detailed, professional photography",
            "åŠ¨æ¼«": "anime style, manga, cel shading, vibrant colors",
            "æ²¹ç”»": "oil painting, classical art style, brush strokes",
            "æ°´å½©": "watercolor painting, soft colors, artistic",
            "ç´ æ": "pencil sketch, black and white, detailed drawing",
            "å¡é€š": "cartoon style, colorful, simple shapes",
            "ç§‘å¹»": "sci-fi, futuristic, cyberpunk, neon lights",
            "æ¢¦å¹»": "dreamy, surreal, ethereal, magical"
        }
    
    def _check_dependencies(self):
        """æ£€æŸ¥å›¾åƒç”Ÿæˆä¾èµ–"""
        if not self.diffusers_available:
            raise ImportError(
                "å›¾åƒç”ŸæˆåŠŸèƒ½éœ€è¦é¢å¤–ä¾èµ–ï¼Œè¯·è¿è¡Œ: pip install -r requirements-image.txt"
            )
    
    def load_pipeline(self, model_name: str = "sd15"):
        """åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹"""
        self._check_dependencies()
        
        if self.pipeline is not None:
            return
        
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½å›¾åƒç”Ÿæˆæ¨¡å‹: {model_name}")
        
        try:
            model_id = self.default_models.get(model_name, model_name)
            
            if model_name == "sdxl":
                self.pipeline = StableDiffusionXLPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    use_safetensors=True,
                    variant="fp16" if self.device == "cuda" else None
                )
            else:
                self.pipeline = StableDiffusionPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    use_safetensors=True
                )
            
            # ä¼˜åŒ–è®¾ç½®
            if self.device == "cuda":
                self.pipeline = self.pipeline.to(self.device)
                self.pipeline.enable_memory_efficient_attention()
                try:
                    self.pipeline.enable_xformers_memory_efficient_attention()
                except:
                    pass
            
            # è®¾ç½®è°ƒåº¦å™¨
            self.pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
                self.pipeline.scheduler.config
            )
            
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def optimize_prompt(self, user_prompt: str, language: str = "zh") -> str:
        """ä½¿ç”¨LLMä¼˜åŒ–å›¾åƒç”Ÿæˆæç¤ºè¯"""
        if language == "zh":
            optimization_prompt = f"""
è¯·å°†ä»¥ä¸‹ä¸­æ–‡æè¿°è½¬æ¢ä¸ºé€‚åˆAIå›¾åƒç”Ÿæˆçš„è‹±æ–‡æç¤ºè¯ï¼š

ç”¨æˆ·æè¿°ï¼š{user_prompt}

è¦æ±‚ï¼š
1. è½¬æ¢ä¸ºè¯¦ç»†çš„è‹±æ–‡æè¿°
2. åŒ…å«å…·ä½“çš„è§†è§‰å…ƒç´ ï¼ˆé¢œè‰²ã€å…‰çº¿ã€æ„å›¾ç­‰ï¼‰
3. æ·»åŠ ç”»è´¨ç›¸å…³è¯æ±‡ï¼ˆå¦‚high quality, detailed, 8kç­‰ï¼‰
4. ä¿æŒåŸæ„ä¸å˜ï¼Œä½†è®©æè¿°æ›´åŠ å…·ä½“å’Œç”ŸåŠ¨
5. ä½¿ç”¨é€—å·åˆ†éš”ä¸åŒçš„æè¿°å…ƒç´ 

è¯·åªè¿”å›ä¼˜åŒ–åçš„è‹±æ–‡æç¤ºè¯ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šï¼š
"""
        else:
            optimization_prompt = f"""
Optimize the following prompt for AI image generation:

User prompt: {user_prompt}

Requirements:
1. Make it more detailed and specific
2. Add visual elements (colors, lighting, composition)
3. Include quality keywords (high quality, detailed, 8k, etc.)
4. Use comma-separated keywords
5. Keep the original meaning

Return only the optimized prompt:
"""
        
        try:
            optimized = self.llm_client.generate(
                optimization_prompt, 
                max_tokens=200,
                temperature=0.7
            )
            return optimized.strip()
        except Exception as e:
            print(f"æç¤ºè¯ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æç¤ºè¯: {e}")
            return user_prompt
    
    def generate_image(
        self,
        prompt: str,
        style: str = "å†™å®",
        width: int = 512,
        height: int = 512,
        num_images: int = 1,
        steps: int = 20,
        guidance_scale: float = 7.5,
        negative_prompt: str = None,
        seed: int = None,
        optimize_prompt: bool = True,
        model_name: str = "sd15"
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå›¾åƒ"""
        self._check_dependencies()
        
        # åŠ è½½æ¨¡å‹
        if self.pipeline is None:
            self.load_pipeline(model_name)
        
        # ä¼˜åŒ–æç¤ºè¯
        if optimize_prompt:
            print("ğŸ”„ æ­£åœ¨ä¼˜åŒ–æç¤ºè¯...")
            prompt = self.optimize_prompt(prompt)
            print(f"âœ¨ ä¼˜åŒ–åçš„æç¤ºè¯: {prompt}")
        
        # æ·»åŠ é£æ ¼
        style_addition = self.style_prompts.get(style, "")
        if style_addition:
            prompt = f"{prompt}, {style_addition}"
        
        # é»˜è®¤è´Ÿé¢æç¤ºè¯
        if negative_prompt is None:
            negative_prompt = "blurry, low quality, distorted, deformed, watermark, text"
        
        # è®¾ç½®éšæœºç§å­
        if seed is not None:
            torch.manual_seed(seed)
            np.random.seed(seed)
        else:
            seed = np.random.randint(0, 2**32 - 1)
            torch.manual_seed(seed)
        
        print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
        print(f"ğŸ“ æç¤ºè¯: {prompt}")
        print(f"ğŸ­ é£æ ¼: {style}")
        print(f"ğŸ“ å°ºå¯¸: {width}x{height}")
        print(f"ğŸ¯ ç”Ÿæˆæ•°é‡: {num_images}")
        
        try:
            # ç”Ÿæˆå‚æ•°
            generation_kwargs = {
                "prompt": prompt,
                "negative_prompt": negative_prompt,
                "num_images_per_prompt": num_images,
                "num_inference_steps": steps,
                "guidance_scale": guidance_scale,
                "width": width,
                "height": height,
                "generator": torch.Generator(device=self.device).manual_seed(seed)
            }
            
            # ç”Ÿæˆå›¾åƒ
            result = self.pipeline(**generation_kwargs)
            images = result.images
            
            # ä¿å­˜å›¾åƒ
            saved_paths = []
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            for i, image in enumerate(images):
                filename = f"{timestamp}_img_{i+1}_seed_{seed}.png"
                filepath = os.path.join(self.output_dir, filename)
                image.save(filepath)
                saved_paths.append(filepath)
            
            generation_info = {
                "images": images,
                "saved_paths": saved_paths,
                "metadata": {
                    "prompt": prompt,
                    "negative_prompt": negative_prompt,
                    "style": style,
                    "width": width,
                    "height": height,
                    "num_images": num_images,
                    "steps": steps,
                    "guidance_scale": guidance_scale,
                    "seed": seed,
                    "model_name": model_name,
                    "generated_at": datetime.now().isoformat()
                }
            }
            
            print(f"âœ… å›¾åƒç”Ÿæˆå®Œæˆ! ä¿å­˜åˆ°: {saved_paths}")
            return generation_info
            
        except Exception as e:
            print(f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
            raise e
    
    def generate_batch_images(
        self,
        prompts: List[str],
        **kwargs
    ) -> Dict[str, Any]:
        """æ‰¹é‡ç”Ÿæˆå›¾åƒ"""
        results = []
        failed_prompts = []
        
        for i, prompt in enumerate(prompts):
            print(f"\nğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(prompts)} ä¸ªæç¤ºè¯")
            try:
                result = self.generate_image(prompt, **kwargs)
                results.append({
                    "prompt": prompt,
                    "result": result
                })
            except Exception as e:
                print(f"âŒ æç¤ºè¯ '{prompt}' ç”Ÿæˆå¤±è´¥: {e}")
                failed_prompts.append({
                    "prompt": prompt,
                    "error": str(e)
                })
        
        return {
            "successful_results": results,
            "failed_prompts": failed_prompts,
            "total_processed": len(prompts),
            "success_count": len(results),
            "failure_count": len(failed_prompts)
        }
    
    def create_image_grid(self, images: List[Image.Image], grid_size: Tuple[int, int] = None) -> Image.Image:
        """åˆ›å»ºå›¾åƒç½‘æ ¼"""
        if not images:
            return None
        
        # è‡ªåŠ¨è®¡ç®—ç½‘æ ¼å¤§å°
        if grid_size is None:
            cols = int(np.ceil(np.sqrt(len(images))))
            rows = int(np.ceil(len(images) / cols))
            grid_size = (cols, rows)
        
        cols, rows = grid_size
        
        # è·å–å•ä¸ªå›¾åƒå°ºå¯¸
        img_width, img_height = images[0].size
        
        # åˆ›å»ºç½‘æ ¼ç”»å¸ƒ
        grid_width = cols * img_width
        grid_height = rows * img_height
        grid_image = Image.new('RGB', (grid_width, grid_height), color='white')
        
        # ç²˜è´´å›¾åƒåˆ°ç½‘æ ¼
        for idx, image in enumerate(images):
            if idx >= cols * rows:
                break
            
            row = idx // cols
            col = idx % cols
            
            x = col * img_width
            y = row * img_height
            
            grid_image.paste(image, (x, y))
        
        return grid_image
    
    def save_generation_info(self, generation_info: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜ç”Ÿæˆä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{timestamp}_generation_info.json"
        
        filepath = os.path.join(self.output_dir, filename)
        
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„æ•°æ®
        serializable_info = generation_info.copy()
        serializable_info.pop('images', None)  # ç§»é™¤PILå›¾åƒå¯¹è±¡
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(serializable_info, f, ensure_ascii=False, indent=2)
        
        return filepath
    
    def cleanup_model(self):
        """æ¸…ç†æ¨¡å‹é‡Šæ”¾æ˜¾å­˜"""
        if self.pipeline is not None:
            del self.pipeline
            self.pipeline = None
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print("ğŸ§¹ æ¨¡å‹å·²æ¸…ç†ï¼Œæ˜¾å­˜å·²é‡Šæ”¾")